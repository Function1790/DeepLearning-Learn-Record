{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\이재헌\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(4.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "@tf.function\n",
    "def hypothesis(x):\n",
    "  return w*x+b\n",
    "\n",
    "@tf.function\n",
    "def mse_loss(y_pred, y):\n",
    "  # 두 개의 차이값을 제곱을 해서 평균을 취한다.\n",
    "  return tf.reduce_mean(tf.square(y_pred - y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5. 7. 9.]\n",
      "13.5\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [1, 2, 3, 4]\n",
    "\n",
    "x_test = [1, 2, 3, 4]\n",
    "y_pred = hypothesis(x_test)\n",
    "loss = mse_loss(y_pred, y)\n",
    "print(y_pred.numpy())\n",
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   0 | w의 값 : 10.8033 | b의 값 : 0.8211 | cost : 1.273473\n",
      "epoch :  10 | w의 값 : 10.8732 | b의 값 : 0.7979 | cost : 0.135148\n",
      "epoch :  20 | w의 값 : 10.8783 | b의 값 : 0.7657 | cost : 0.124468\n",
      "epoch :  30 | w의 값 : 10.8832 | b의 값 : 0.7348 | cost : 0.114633\n",
      "epoch :  40 | w의 값 : 10.8879 | b의 값 : 0.7052 | cost : 0.105575\n",
      "epoch :  50 | w의 값 : 10.8924 | b의 값 : 0.6768 | cost : 0.097231\n",
      "epoch :  60 | w의 값 : 10.8968 | b의 값 : 0.6495 | cost : 0.089548\n",
      "epoch :  70 | w의 값 : 10.9009 | b의 값 : 0.6233 | cost : 0.082473\n",
      "epoch :  80 | w의 값 : 10.9049 | b의 값 : 0.5981 | cost : 0.075955\n",
      "epoch :  90 | w의 값 : 10.9088 | b의 값 : 0.574 | cost : 0.069953\n",
      "epoch : 100 | w의 값 : 10.9125 | b의 값 : 0.5509 | cost : 0.064425\n",
      "epoch : 110 | w의 값 : 10.9160 | b의 값 : 0.5287 | cost : 0.059335\n",
      "epoch : 120 | w의 값 : 10.9194 | b의 값 : 0.5073 | cost : 0.054646\n",
      "epoch : 130 | w의 값 : 10.9226 | b의 값 : 0.4869 | cost : 0.050328\n",
      "epoch : 140 | w의 값 : 10.9257 | b의 값 : 0.4673 | cost : 0.046351\n",
      "epoch : 150 | w의 값 : 10.9287 | b의 값 : 0.4484 | cost : 0.042687\n",
      "epoch : 160 | w의 값 : 10.9316 | b의 값 : 0.4303 | cost : 0.039315\n",
      "epoch : 170 | w의 값 : 10.9344 | b의 값 : 0.413 | cost : 0.036208\n",
      "epoch : 180 | w의 값 : 10.9370 | b의 값 : 0.3963 | cost : 0.033347\n",
      "epoch : 190 | w의 값 : 10.9396 | b의 값 : 0.3803 | cost : 0.030712\n",
      "epoch : 200 | w의 값 : 10.9420 | b의 값 : 0.365 | cost : 0.028285\n",
      "epoch : 210 | w의 값 : 10.9443 | b의 값 : 0.3503 | cost : 0.026050\n",
      "epoch : 220 | w의 값 : 10.9466 | b의 값 : 0.3362 | cost : 0.023991\n",
      "epoch : 230 | w의 값 : 10.9487 | b의 값 : 0.3226 | cost : 0.022096\n",
      "epoch : 240 | w의 값 : 10.9508 | b의 값 : 0.3096 | cost : 0.020349\n",
      "epoch : 250 | w의 값 : 10.9528 | b의 값 : 0.2971 | cost : 0.018741\n",
      "epoch : 260 | w의 값 : 10.9547 | b의 값 : 0.2851 | cost : 0.017261\n",
      "epoch : 270 | w의 값 : 10.9565 | b의 값 : 0.2736 | cost : 0.015896\n",
      "epoch : 280 | w의 값 : 10.9583 | b의 값 : 0.2626 | cost : 0.014640\n",
      "epoch : 290 | w의 값 : 10.9599 | b의 값 : 0.252 | cost : 0.013483\n",
      "epoch : 300 | w의 값 : 10.9616 | b의 값 : 0.2419 | cost : 0.012418\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간\n",
    "y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적\n",
    "\n",
    "optimizer = tf.optimizers.SGD(0.01)\n",
    "for i in range(301):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
    "    y_pred = hypothesis(x)\n",
    "\n",
    "    # 평균 제곱 오차를 계산\n",
    "    cost = mse_loss(y_pred, y)\n",
    "\n",
    "  # 손실 함수에 대한 파라미터의 미분값 계산\n",
    "  gradients = tape.gradient(cost, [w, b])\n",
    "\n",
    "  # 파라미터 업데이트\n",
    "  optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(\"epoch : {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, w.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.203414 22.164978 33.12654  44.088104 55.049667 66.01123  76.97279\n",
      " 87.93436  98.89592 ]\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis(x).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
